{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dxp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/tmp/speech_commands_train; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1f64181f111d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model/speech_commands_train/conv.ckpt-16200.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model/speech_commands_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mlatest_checkpoint\u001b[0;34m(checkpoint_dir, latest_filename)\u001b[0m\n\u001b[1;32m    341\u001b[0m     v1_path = _prefix_to_checkpoint_path(ckpt.model_checkpoint_path,\n\u001b[1;32m    342\u001b[0m                                          saver_pb2.SaverDef.V1)\n\u001b[0;32m--> 343\u001b[0;31m     if file_io.get_matching_files(v2_path) or file_io.get_matching_files(\n\u001b[0m\u001b[1;32m    344\u001b[0m         v1_path):\n\u001b[1;32m    345\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mlisting\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m   \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_matching_files_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    387\u001b[0m           \u001b[0;31m# Convert the filenames to string from bytes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m           for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[1;32m    391\u001b[0m               compat.as_bytes(single_filename), status)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /tmp/speech_commands_train; No such file or directory"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('../model/speech_commands_train/conv.ckpt-16200.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('../model/speech_commands_train'))\n",
    "    var_list = {v.name: sess.run(v) for v in tf.global_variables()}\n",
    "    for i in var_list.items():\n",
    "        print(i[0], i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/speech_commands_train/conv.ckpt-16200\n",
      "first_weights:0 (20, 8, 1, 64)\n",
      "first_bias:0 (64,)\n",
      "second_weights:0 (10, 4, 64, 64)\n",
      "second_bias:0 (64,)\n",
      "final_fc_weights:0 (30720, 7)\n",
      "final_fc_bias:0 (7,)\n",
      "global_step:0 ()\n"
     ]
    }
   ],
   "source": [
    "saver1 = tf.train.Saver()\n",
    "with tf.Session() as sess:   \n",
    "    saver1.restore(sess,\"/tmp/speech_commands_train/conv.ckpt-16200\")\n",
    "    var_list = {v.name: sess.run(v) for v in tf.global_variables()}\n",
    "    for i in var_list.items():\n",
    "        print(i[0], i[1].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'desired_samples': 24000.0, 'window_size_samples': 1440.0, 'window_stride_samples': 480.0, 'spectrogram_length': 48, 'fingerprint_width': 40, 'fingerprint_size': 1920, 'label_count': 7, 'sample_rate': 48000, 'preprocess': 'mfcc', 'average_window_width': -1}\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 48000\n",
    "desired_samples = sample_rate*0.5\n",
    "window_size_samples = sample_rate*0.03\n",
    "window_stride_samples = sample_rate*0.01\n",
    "length_minus_window = (desired_samples - window_size_samples)\n",
    "spectrogram_length = 1 + int(length_minus_window / window_stride_samples)\n",
    "fingerprint_width = 40\n",
    "fingerprint_size = spectrogram_length * fingerprint_width\n",
    "label_count = 7\n",
    "preprocess = 'mfcc'\n",
    "average_window_width = -1\n",
    "\n",
    "model_settings = {\n",
    "      'desired_samples': desired_samples,\n",
    "      'window_size_samples': window_size_samples,\n",
    "      'window_stride_samples': window_stride_samples,\n",
    "      'spectrogram_length': spectrogram_length,\n",
    "      'fingerprint_width': fingerprint_width,\n",
    "      'fingerprint_size': fingerprint_size,\n",
    "      'label_count': label_count,\n",
    "      'sample_rate': sample_rate,\n",
    "      'preprocess': preprocess,\n",
    "      'average_window_width': average_window_width,\n",
    "  }\n",
    "print(model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_weights:0 <tf.Variable 'first_weights:0' shape=(20, 8, 1, 64) dtype=float32_ref>\n",
      "first_bias:0 <tf.Variable 'first_bias:0' shape=(64,) dtype=float32_ref>\n",
      "second_weights:0 <tf.Variable 'second_weights:0' shape=(10, 4, 64, 64) dtype=float32_ref>\n",
      "second_bias:0 <tf.Variable 'second_bias:0' shape=(64,) dtype=float32_ref>\n",
      "final_fc_weights:0 <tf.Variable 'final_fc_weights:0' shape=(30720, 7) dtype=float32_ref>\n",
      "final_fc_bias:0 <tf.Variable 'final_fc_bias:0' shape=(7,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "#模型保存, placeholder放在fingerprint\n",
    "g1 = tf.Graph()\n",
    "import models\n",
    "with g1.as_default():\n",
    "    input_placeholder = tf.placeholder(\n",
    "        tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "    logits = models.create_model(\n",
    "        input_placeholder,\n",
    "        model_settings,\n",
    "        'conv',\n",
    "        is_training=False)\n",
    "    var_list2 = {v.name: v for v in tf.global_variables()}\n",
    "    for i in var_list2.items():\n",
    "        print(i[0], i[1])\n",
    "    var_op = {tf.assign(v[1],var_list[v[0]]) for v in var_list2.items()}\n",
    "    \n",
    "with tf.Session(graph=g1) as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    for i in var_op:\n",
    "        sess2.run(i)\n",
    "        converter = tf.lite.TFLiteConverter.from_session(sess2,\n",
    "                                                         [input_placeholder],\n",
    "                                                         [logits])\n",
    "        tflite_model = converter.convert()\n",
    "        open(\"fingerprint_input_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(24000, 1)\n"
     ]
    }
   ],
   "source": [
    "#测试输出\n",
    "filename = \"../data/98K/audio_75490.688279.wav\"\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
    "    wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
    "    wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)\n",
    "    b = sess.run(wav_decoder,\n",
    "                 feed_dict={wav_filename_placeholder: filename}).audio\n",
    "    print(type(b))\n",
    "    print(b.shape)\n",
    "    str_b = ','.join(['%0.10f' %i[0] for i in b ])\n",
    "    with open('audio.txt','wt') as f:\n",
    "        f.write(str_b + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_weights:0 <tf.Variable 'first_weights:0' shape=(20, 8, 1, 64) dtype=float32_ref>\n",
      "first_bias:0 <tf.Variable 'first_bias:0' shape=(64,) dtype=float32_ref>\n",
      "second_weights:0 <tf.Variable 'second_weights:0' shape=(10, 4, 64, 64) dtype=float32_ref>\n",
      "second_bias:0 <tf.Variable 'second_bias:0' shape=(64,) dtype=float32_ref>\n",
      "final_fc_weights:0 <tf.Variable 'final_fc_weights:0' shape=(30720, 7) dtype=float32_ref>\n",
      "final_fc_bias:0 <tf.Variable 'final_fc_bias:0' shape=(7,) dtype=float32_ref>\n",
      "[[-4.615737   -5.4014144   7.51701    -0.9978459   1.8207328   3.0950725\n",
      "   0.69415474]]\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "#模型保存，placeholder放在audio_data\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    audio_data_placeholder = tf.placeholder(shape=[24000,1],dtype=tf.float32,name=\"autdio_ph\")\n",
    "    spectrogram = contrib_audio.audio_spectrogram(\n",
    "        audio_data_placeholder,\n",
    "        window_size=model_settings['window_size_samples'],\n",
    "        stride=model_settings['window_stride_samples'],\n",
    "        magnitude_squared=True)\n",
    "    mfcc = contrib_audio.mfcc(\n",
    "        spectrogram,\n",
    "        model_settings['sample_rate'],\n",
    "        dct_coefficient_count=model_settings['fingerprint_width'])\n",
    "    shape_mfcc = tf.reshape(mfcc,shape=(1,model_settings['fingerprint_size']))\n",
    "    logits = models.create_model(\n",
    "        shape_mfcc,\n",
    "        model_settings,\n",
    "        'conv',\n",
    "        is_training=False)\n",
    "    var_list3 = {v.name: v for v in tf.global_variables()}\n",
    "    for i in var_list3.items():\n",
    "        print(i[0], i[1])\n",
    "    var_op = {tf.assign(v[1],var_list[v[0]]) for v in var_list3.items()}\n",
    "    \n",
    "    \n",
    "with tf.Session(graph=g2) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in var_op:\n",
    "        sess.run(i)\n",
    "    res = sess.run(logits, feed_dict={audio_data_placeholder: b})\n",
    "    print(res)\n",
    "    \n",
    "    \n",
    "with tf.Session(graph=g2) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in var_op:\n",
    "        sess.run(i)\n",
    "        \n",
    "    from tensorflow.lite.python.convert import OpsSet\n",
    "    converter = tf.lite.TFLiteConverter.from_session(sess,[audio_data_placeholder],[logits])\n",
    "    converter.target_ops = set([OpsSet.TFLITE_BUILTINS,OpsSet.SELECT_TF_OPS])\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"audio_input_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
