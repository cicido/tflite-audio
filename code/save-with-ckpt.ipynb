{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #注意将模型移动/tmp目录下.\n",
    "    saver = tf.train.import_meta_graph('/tmp/speech_commands_train/conv.ckpt-16200.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('/tmp/speech_commands_train'))\n",
    "    var_list = {v.name: sess.run(v) for v in tf.global_variables()}\n",
    "    for i in var_list.items():\n",
    "        print(i[0], i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver1 = tf.train.Saver()\n",
    "with tf.Session() as sess:   \n",
    "    saver1.restore(sess,\"/tmp/speech_commands_train/conv.ckpt-16200\")\n",
    "    var_list = {v.name: sess.run(v) for v in tf.global_variables()}\n",
    "    for i in var_list.items():\n",
    "        print(i[0], i[1].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 48000\n",
    "desired_samples = sample_rate*0.5\n",
    "window_size_samples = sample_rate*0.03\n",
    "window_stride_samples = sample_rate*0.01\n",
    "length_minus_window = (desired_samples - window_size_samples)\n",
    "spectrogram_length = 1 + int(length_minus_window / window_stride_samples)\n",
    "fingerprint_width = 40\n",
    "fingerprint_size = spectrogram_length * fingerprint_width\n",
    "label_count = 7\n",
    "preprocess = 'mfcc'\n",
    "average_window_width = -1\n",
    "\n",
    "model_settings = {\n",
    "      'desired_samples': desired_samples,\n",
    "      'window_size_samples': window_size_samples,\n",
    "      'window_stride_samples': window_stride_samples,\n",
    "      'spectrogram_length': spectrogram_length,\n",
    "      'fingerprint_width': fingerprint_width,\n",
    "      'fingerprint_size': fingerprint_size,\n",
    "      'label_count': label_count,\n",
    "      'sample_rate': sample_rate,\n",
    "      'preprocess': preprocess,\n",
    "      'average_window_width': average_window_width,\n",
    "  }\n",
    "print(model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存, placeholder放在fingerprint\n",
    "g1 = tf.Graph()\n",
    "import models\n",
    "with g1.as_default():\n",
    "    input_placeholder = tf.placeholder(\n",
    "        tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "    logits = models.create_model(\n",
    "        input_placeholder,\n",
    "        model_settings,\n",
    "        'conv',\n",
    "        is_training=False)\n",
    "    var_list2 = {v.name: v for v in tf.global_variables()}\n",
    "    for i in var_list2.items():\n",
    "        print(i[0], i[1])\n",
    "    var_op = {tf.assign(v[1],var_list[v[0]]) for v in var_list2.items()}\n",
    "    \n",
    "with tf.Session(graph=g1) as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    for i in var_op:\n",
    "        sess2.run(i)\n",
    "        converter = tf.lite.TFLiteConverter.from_session(sess2,\n",
    "                                                         [input_placeholder],\n",
    "                                                         [logits])\n",
    "        tflite_model = converter.convert()\n",
    "        open(\"fingerprint_input_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试输出\n",
    "filename = \"../data/98K/audio_75490.688279.wav\"\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
    "    wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
    "    wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)\n",
    "    b = sess.run(wav_decoder,\n",
    "                 feed_dict={wav_filename_placeholder: filename}).audio\n",
    "    print(type(b))\n",
    "    print(b.shape)\n",
    "    str_b = ','.join(['%0.10f' %i[0] for i in b ])\n",
    "    with open('audio.txt','wt') as f:\n",
    "        f.write(str_b + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存，placeholder放在audio_data\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    audio_data_placeholder = tf.placeholder(shape=[24000,1],dtype=tf.float32,name=\"autdio_ph\")\n",
    "    spectrogram = contrib_audio.audio_spectrogram(\n",
    "        audio_data_placeholder,\n",
    "        window_size=model_settings['window_size_samples'],\n",
    "        stride=model_settings['window_stride_samples'],\n",
    "        magnitude_squared=True)\n",
    "    mfcc = contrib_audio.mfcc(\n",
    "        spectrogram,\n",
    "        model_settings['sample_rate'],\n",
    "        dct_coefficient_count=model_settings['fingerprint_width'])\n",
    "    shape_mfcc = tf.reshape(mfcc,shape=(1,model_settings['fingerprint_size']))\n",
    "    logits = models.create_model(\n",
    "        shape_mfcc,\n",
    "        model_settings,\n",
    "        'conv',\n",
    "        is_training=False)\n",
    "    var_list3 = {v.name: v for v in tf.global_variables()}\n",
    "    for i in var_list3.items():\n",
    "        print(i[0], i[1])\n",
    "    var_op = {tf.assign(v[1],var_list[v[0]]) for v in var_list3.items()}\n",
    "    \n",
    "    \n",
    "with tf.Session(graph=g2) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in var_op:\n",
    "        sess.run(i)\n",
    "    res = sess.run(logits, feed_dict={audio_data_placeholder: b})\n",
    "    print(res)\n",
    "    \n",
    "    \n",
    "with tf.Session(graph=g2) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in var_op:\n",
    "        sess.run(i)\n",
    "        \n",
    "    from tensorflow.lite.python.convert import OpsSet\n",
    "    converter = tf.lite.TFLiteConverter.from_session(sess,[audio_data_placeholder],[logits])\n",
    "    converter.target_ops = set([OpsSet.TFLITE_BUILTINS,OpsSet.SELECT_TF_OPS])\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"audio_input_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
